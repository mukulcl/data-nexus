# Data Nexus â€” Big Data Ingestion & Credit-Based API Platform

Hi ğŸ‘‹, Iâ€™m **Mukul**, the developer behind **Data Nexus**.

This project demonstrates my capabilities in:

* **Big data ingestion** of heterogeneous datasets
* **Scalable storage** for analytical querying
* **API engineering** with authentication & credits
* **End-to-end architecture design** & deployment readiness

My goal was to build a solution that is not only fully functional, but also **production-aligned and cloud-compatible**, even while using completely **free** and **open-source** technologies.

---

## ğŸ§  What the system does

Data Nexus is capable of:

âœ” Ingesting multi-format files: CSV, TSV, XLSX, JSONL, Parquet
âœ” Detecting schema variations & normalizing data
âœ” Deduplication via deterministic checksum
âœ” High-performance querying via **ClickHouse**
âœ” Secure REST API with:

* API-key authentication
* Credit-based access control
* Rate limiting
* Pagination, filters, metadata
  âœ” CSV export for bulk access

---

## âœ” Completed Modules

| Module                                    | Status      |
| ----------------------------------------- | ----------- |
| Project structure + architecture docs     | âœ” Done      |
| Docker Compose stack (API + DBs + worker) | âœ” Done      |
| Postgres schema (users, credits, logs)    | âœ” Done      |
| ClickHouse master schema                  | âœ” Done      |
| Fastify API + TypeScript backend          | âœ” Done      |
| Swagger documentation                     | âœ” Done      |
| API key authentication                    | âœ” Done      |
| Rate limiting (Redis)                     | âœ” Done      |
| Hybrid credit deduction system            | âœ” Done      |
| Query endpoints with filters + metadata   | âœ” Done      |
| CSV export route                          | âœ” Done      |
| Ingestion worker foundation               | âœ” Done      |
| File registry + checksum tracking         | In Progress |
| CSV â†’ ClickHouse ingest path              | In Progress |
| Normalization + full dedupe pipeline      | Pending     |
| Admin credit panel or scripts             | Pending     |

> The ingestion path is running with initial CSV support â€” validation + Parquet bulk-load will extend it further.

---

## ğŸ§° Tech Stack I Used

| Layer                   | Technology           |
| ----------------------- | -------------------- |
| Language                | TypeScript (Node.js) |
| API Framework           | Fastify              |
| OLAP Database           | ClickHouse           |
| Metadata & Credits      | PostgreSQL           |
| Caching & Rate Limiting | Redis                |
| Object Storage          | MinIO                |
| Ingestion               | DuckDB + BullMQ      |
| Deployment              | Docker Compose       |

> The solution can seamlessly migrate to AWS/GCP later
> (S3 / RDS / ElastiCache / ClickHouse Cloud / GCS)

---

## ğŸ” Credit System (Hybrid Model)

Credits are deducted dynamically based on:

`` Base Cost + (Rows Returned Ã— Cost/Row) + (Compute Time Ã— Cost/Sec) ``

Example error when credits run out:

`` json
{ "error": "Insufficient Credits" }
``

---

## ğŸ“¡ API Endpoints (Implemented)

| Endpoint              | Purpose                                |
| --------------------- | -------------------------------------- |
| `GET /health`         | Service status                         |
| `GET /docs`           | Swagger UI                             |
| `GET /v1/records`     | List records with filters + pagination |
| `GET /v1/records/:id` | Single record lookup                   |
| `POST /v1/query`      | Advanced query using JSON body         |
| `GET /v1/export`      | CSV export option                      |

**Every response includes:**
`credits_used`, `response_time`, `total_records`, `pagination`, etc.

---

## ğŸ— Architecture Snapshot

``
Raw Data â†’ MinIO
         â†’ Ingestion Worker (Node + DuckDB + BullMQ)
         â†’ Clean Data â†’ ClickHouse
         â†’ Fastify API â†’ (Auth + Credits + Filters + Pagination)
         â†’ Clients
``

Supporting services:

* Redis for rate limiting & counters
* PostgreSQL for API keys & usage logs

---

## ğŸ“‚ Project Structure

``
/docs           â†’ Architecture & API docs
/src/api        â†’ Fastify API backend
/src/ingestion  â†’ Workers + DuckDB pipeline
/src/utils      â†’ Hashing, config, helpers
/deployment     â†’ Docker Compose & env templates
``

---

## ğŸ§© Whatâ€™s Next (Planned Extensions)

| Priority | Feature                                         |
| -------- | ----------------------------------------------- |
| High     | Complete normalization & validation layer       |
| High     | Parquet export + stable bulk-load to ClickHouse |
| Medium   | Admin credit operations panel                   |
| Medium   | Meilisearch fuzzy search                        |
| Optional | Tests + performance benchmarks                  |
| Optional | Prometheus & Grafana dashboards                 |

---

## ğŸ’¡ Reflection

This project helped me showcase:

* **Real-world workflow of a big-data system**
* Designing scalable infrastructure from scratch
* Working with unfamiliar data situations
* Combining SQL + JavaScript engineering skills
* Prioritizing execution under time constraints

I genuinely enjoyed building this and would love to expand on it further ğŸš€

---

## ğŸ‘¤ Developer

**Mukul**
India
